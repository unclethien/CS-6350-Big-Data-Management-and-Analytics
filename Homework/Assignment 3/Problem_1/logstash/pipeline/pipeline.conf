# Problem_1/logstash/pipeline/pipeline.conf
input {
  kafka {
    bootstrap_servers => "kafka:9092" # Internal Docker network address
    topics => ["news-entities"]
    codec => "json" # Tell Logstash the incoming messages are JSON
    group_id => "logstash_entity_consumer"
    consumer_threads => 1
    auto_offset_reset => "latest" # Start reading from the latest messages
  }
}

filter {
  # The codec => "json" in the input should handle parsing.
  # If fields are nested under a default field (e.g., "message"), uncomment below:
  # json {
  #   source => "message" # Adjust if your JSON is nested under a different field
  # }

  # Ensure 'count' is treated as a number
  mutate {
    convert => {
      "count" => "integer"
    }
  }

  # Optionally parse window start time as the primary timestamp for Kibana
  # This helps if Kibana's time filter should align with Spark's window start.
  # Note: Spark outputs timestamps like 'YYYY-MM-DD HH:MM:SS'. Adjust the pattern if needed.
  date {
     match => [ "window_start", "yyyy-MM-dd HH:mm:ss" ]
     target => "@timestamp"
     # If parsing fails, keep the original Logstash ingest time
  }
}

output {
  elasticsearch {
    hosts => ["http://elasticsearch:9200"] # Internal Docker network address
    index => "news-entities-%{+YYYY.MM.dd}" # Create daily indices
    # manage_template => false # Optional: If you want to manage mapping manually
  }
  # Optional: Output to console for debugging
  # stdout { codec => rubydebug }
}
