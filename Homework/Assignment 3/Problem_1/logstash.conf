# Logstash configuration to read NER results from Kafka and send to Elasticsearch

input {
  kafka {
    bootstrap_servers => "localhost:9092" # Your Kafka broker list
    topics => ["ner_results_topic"]      # Kafka topic with NER results
    group_id => "logstash_ner_consumer" # Consumer group ID
    codec => "json"                     # Messages are in JSON format
    # Adjust consumer settings as needed (e.g., auto_offset_reset)
    # auto_offset_reset => "latest"
  }
}

filter {
  # No complex filtering needed here as Spark already processed the data
  # We assume the JSON from Kafka is { "entity": "some_entity", "count": 123 }
  # Logstash's json codec in the input handles parsing.

  # Optional: Convert count to integer if needed by Elasticsearch mapping
  mutate {
    convert => {
      "count" => "integer"
    }
  }
}

output {
  elasticsearch {
    hosts => ["localhost:9200"] # Your Elasticsearch host(s)
    index => "ner-counts-%{+YYYY.MM.dd}" # Index name pattern (daily indices)
    document_id => "%{entity}" # Use the entity name as the document ID to update counts
    # If using security features in Elasticsearch:
    # user => "elastic"
    # password => "your_password"
  }

  # Optional: Output to console for debugging
  # stdout {
  #   codec => rubydebug
  # }
}
