version: "3.8"
services:
  zookeeper:
    image: bitnami/zookeeper:latest # ‚Üê use latest or another real tag
    container_name: zookeeper
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes

  kafka:
    image: bitnami/kafka:3.4.0-debian-11-r2 # << works with ZooKeeper
    container_name: kafka
    depends_on:
      - zookeeper
    environment:
      - KAFKA_CFG_KRAFT_MODE=false
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_CFG_BROKER_ID=1
      # - KAFKA_CFG_NODE_ID=1 # Removed, KRaft specific
      # - KAFKA_CFG_PROCESS_ROLES=broker # Removed, KRaft specific
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092
      - ALLOW_PLAINTEXT_LISTENER=yes
    ports:
      - "9092:9092"

  news-producer:
    build:
      context: .
      dockerfile: news-producer/Dockerfile
    container_name: news-producer
    depends_on:
      - kafka
    # Add a command to wait for Kafka to be ready before starting the producer
    command: >
      sh -c "
        echo 'Waiting for Kafka to be ready...'
        sleep 30
        echo 'Starting news producer...'
        python -u /app/finnhub_producer.py 2>&1
      "
    environment:
      # You need a valid Finnhub API key for this to work
      # Sign up at https://finnhub.io/ if you don't have one
      - FINNHUB_API_KEY=${FINNHUB_API_KEY}
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - NEWS_TOPIC=news_raw
    restart: always

  spark-streaming:
    build:
      context: .
      dockerfile: spark-app/Dockerfile
    container_name: spark-streaming
    depends_on:
      - kafka
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - INPUT_TOPIC=news_raw
      - OUTPUT_TOPIC=entity_counts
    restart: on-failure

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.17.10
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    ports:
      - "9200:9200"

  kibana:
    image: docker.elastic.co/kibana/kibana:7.17.10
    container_name: kibana
    depends_on:
      - elasticsearch
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    ports:
      - "5601:5601"

  logstash:
    image: docker.elastic.co/logstash/logstash:7.17.10
    container_name: logstash
    depends_on:
      - kafka
      - elasticsearch
    volumes:
      - ./logstash.conf:/usr/share/logstash/pipeline/logstash.conf:ro
